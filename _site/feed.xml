<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shehel Yoosuf</title>
    <description>hello</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 06 Apr 2018 00:21:31 +0530</pubDate>
    <lastBuildDate>Fri, 06 Apr 2018 00:21:31 +0530</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>Uncertainty in Deep Neural Networks</title>
        <description>&lt;p&gt;Although our world is most probably deterministic, humans by nature operate under imperfect information and uncertainty. And it’s not surprising that we thrive because our biological machinery is so good at dealing with uncertainty helping generate representations of the world that then guide our actions. Quantifying uncertainty is then a reasonable thing to look for in our computational learning models to which we are increasingly offloading decision makings tasks. See AI safety and related case studies if that’s not convincing.&lt;/p&gt;

&lt;p&gt;This will be an ongoing project where I shall try to explore ways of quantifying uncertainty in deep learning models but pardon me when I cut corners because the breadth of the subject is massive. I shall try to compensate this with a thorough reference section. Deep Neural Networks are the models of interest because they have proven to be the state of the art in almost any learning task one can think of. However, its amazing architectural and algorithmic progress has not been consistent with probabilistic models that comes with uncertainty information and all the other benefits of a mathematically elegant formulation. Nevertheless, Bayesian treatment of deep learning(neural network) models has been making a strong comeback since the ‘Golden Era’ in the early ’90s.&lt;/p&gt;

&lt;h2&gt;Classifying Uncertainty&lt;/h2&gt;
&lt;p&gt;There are different classes of uncertainty and I found that there’s a lack of concensus on the terminology. The wikipedia page on &lt;a href=&quot;https://en.wikipedia.org/wiki/Uncertainty_quantification#Sources_of_uncertainty&quot;&gt;uncertainty quantification&lt;/a&gt; does a good job of describing some of these terms. Kendall and Gal(&lt;a href=&quot;https://arxiv.org/pdf/1703.04977.pdf&quot;&gt;2017&lt;/a&gt;) talk about Epistemic and Aleatoric uncertainty which is sufficient for our purposes although I wish these concepts had less imaginative names.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Aleatoric uncertainty, also referred to as the risk in a task is the uncertainty inherent in the task. For example, sensor data are noisy by nature and this can't be fixed by more data. Ofcourse one could improve the sensor and explain away the uncertainty but then it's also a perfect world.&lt;/li&gt;
	&lt;li&gt;Epistemic uncertainty, also referred to as the model uncertainty which is usually the uncertainty in the weights of our models. This can be reduced by enough data(parameter uncertainty) and a model with an adequate structure(layers, non-linearities etc) which is able to capture the complexity of the data generating process.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;But we can simplify our scope to focus only on the predictive probabilities and its uncertainty which encapsulates both the uncertainties above. This suffices for classification tasks we are about to examine, but may not be the case for reinforcement learning and active learning tasks(&lt;a href=&quot;http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html&quot;&gt;Gal&lt;/a&gt;). Disentangling the two is a topic I leave for another post, perhaps &lt;a href=&quot;https://towardsdatascience.com/building-a-bayesian-deep-learning-classifier-ece1845bc09&quot;&gt;this&lt;/a&gt;?. The question this post deals with is to assess ways of making deep learning models provide uncertainty estimates. This is important because they can be the pointers to better models and consequently safe and smarter AI.&lt;/p&gt;

&lt;h2&gt;Methodology&lt;/h2&gt;
&lt;p&gt;I’ll be sticking to the MNIST dataset for the experiments and they can be reproduced by following the Jupyter notebooks &lt;a href=&quot;https://github.com/shehel/Bayesian_analysis&quot;&gt;here&lt;/a&gt;. The quality of the uncertainty estimates will be evaluated by plotting the predictive distribution using samples from the distribution and out of the distribution using the notMNIST &lt;a href=&quot;https://github.com/davidflanagan/notMNIST-to-MNIST&quot;&gt;dataset&lt;/a&gt;. I will be making use of the entropy of the predictive distribution to evaluate uncertainty represented by a model. We seek a model with low entropy and high accuracy on predictive distribution of MNIST and high entropy on notMNIST. We will be using a very basic CNN as shown in the figure below where outputs of each layer is labelled.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/uncertainty/MNISTarch.png&quot; alt=&quot;CNN&quot; title=&quot;CNN&quot; /&gt;&lt;figcaption&gt;CNN Architecture&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;This project already has a serious problem in that there are no benchmark or metric in my knowledge which  can be used to guide the experiments. Yes, I use two opposing datasets and work towards balancing accuracy and entropy but this approach is akin to walking in the dark. For instance, an adequately complex dataset with the ground truth, in this case like labels of a dataset, we also want some kind of information to verify the ‘approximation quality’/uncertainty of the models but I guess a problem, perhaps simpler than MNIST needs to be modelled almost perfectly to get to that. I mention this because a discussion in NIPS 2016 spoke about Yarin and his thesis where he shows an experiment in which an image(a digit in MNIST) is rotated and the predictions not only vary but also with wild uncertainty estimates. This raises some fundamental questions regarding our approach, how far off are we from a good representation of information?. Nevertheless, I’ll use Yarin’s method for further evaluation and put the results up as model uncertainties get ‘better’. Adverserial samples is another way of testing the robustness of models.&lt;/p&gt;

&lt;p&gt;Since this is a young field of research and my lack of expertise means that I won’t be making strong conclusions from evaluating the methods, unless there’s consensus in the research community.&lt;/p&gt;

&lt;h2&gt;Variational Inference with Edward&lt;/h2&gt;
&lt;p&gt;Starting with the most Bayesian option, we will be using a probabilistic platform build around Tensorflow called Edward. Edward lets us define probability distributions for variables. Realisations will be samples from the defined distributions and the library also provides algorithms for probabilistic inference. The difficulty with probabilistic models is the several intractable integrals involved in doing inference. Variational Inference(VI) is a method by which Bayesian inference can be recasted as an optimisation problem, so no more impossible integrals.&lt;/p&gt;

&lt;p&gt;In a nutshell, we will be approximating the true posterior $p(\mathbf{w} \vert \mathbf{X, Y})$ with a simpler distribution $q_{\theta}(\mathbf{w})$ parametrised by $\theta$, in our case it will be Gaussians. We will then minimise the Kullback-Leibler(KL), which is a method to calculate ‘distance’ between two ditributions divergence between the true posterior and the approximate posterior. Edward uses the mean-field VI which assumes the variational distribution over the latent variables factorizes as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(w_1, w_2..w_n) = \prod_{j=1}^{n}q(w_j)&lt;/script&gt;

&lt;p&gt;This is a handicap as now our distributions are unimodal and independent between layers which is not true in the case of neural &lt;u&gt;networks&lt;/u&gt; but it helps computation and it scales to an extent. Minimising the KL divergence is still hard, so we need to move the elements around a bit and we obtain an equivalent quantity to maximise, the Evidence Lower Bound(ELBO) which is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{ELBO} = \underbrace{\mathbb{E}_{q_{\theta}}[\log p(\mathbf{x} \vert \mathbf{z})]}_\text{Expected Log Likelihood} - \underbrace{ \text{KL}( q_{\theta}(\mathbf{z}) \| p(\mathbf{z})] }_\text{Prior KL}&lt;/script&gt;

&lt;p&gt;Implementing a Bayesian Neural Network(BNN) is straightforward in Edward. We don’t need a loss function and regularisation comes for free. However, this doesn’t mean that we get back perfect models as we will soon see. To get predictions from a BNN after training, we can draw samples from the weight distributions to get a collection of models and average their predictions to get a point estimate or we could look at the distribution and evaluate the uncertainty.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/uncertainty/50BNN.png&quot; alt=&quot;50PredictiveDistributions&quot; title=&quot;Distribution of Predictive Accruracies - 50&quot; /&gt;&lt;figcaption&gt;Distribution of accuracies of 50 models sampled from posterior&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Averaging predictions of 50 models gets us to 95% accuracy. Although our model seem to be reaonable with the test set, it’s quite confident with the out of distribution samples as well. The title of the plot contains the test dataset and the mean entropy whereby 0 represents absolute certainty. How come we fall short?&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/uncertainty/MNISTvi.png&quot; alt=&quot;EntropyBNN&quot; title=&quot;Predictive Distribution Entropy&quot; /&gt;&lt;figcaption&gt;Entropy of predictive distribution - MNIST and nMNIST&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Molchanov et al.(&lt;a href=&quot;&quot;&gt;2017&lt;/a&gt;), Trippe and Turner(&lt;a href=&quot;http://approximateinference.org/2017/accepted/TrippeTurner2017.pdf&quot;&gt;2018&lt;/a&gt;) provide compelling evidence of overpruning in mean field variational family. In short, the variability is squashed and the network becomes unable to model uncertainty, therefore variability is traded for sparsity. Trippe and Turner hypothesise that this is due to an unobvious consequence of the ELBO, mean field approximation and balancing modelling the complexity of the data and retaining the simplicity of the prior under this assumption. Note that our approximating distribution which in our case are Gaussians may also not necessarily be anything similar to the true posterior.&lt;/p&gt;

&lt;p&gt;Training on 55,000 images in batches of 128 for 30 epochs took about 14 mins on a 1080Ti, considering it is a minimal CNN, there will be scalability issues with larger CNNs.&lt;/p&gt;

&lt;h2&gt;Monte Carlo(MC) Dropout&lt;/h2&gt;
&lt;p&gt;Dropout is an empirical techniques used to avoid overfitting in neural networks. It does this by randomly switching off a hidden unit and their connection with a probability to prevent hidden units from co-adapting too much. Gal and Gahrmani(&lt;a href=&quot;http://proceedings.mlr.press/v48/gal16.html&quot;&gt;2015&lt;/a&gt;) shows that approximating ELBO and neural networks with dropout are identical. Therefore, optimising any neural network with dropout resembles some manifestation of approximate Bayesian inference.&lt;/p&gt;

&lt;p&gt;In practice, this means that any neural network can be made to resemble Bayesian inference and this allows us to get predictive mean and uncertainty. This is done by applying dropout during test time as well as training in all layers of the network. Different forward passes will effectively give samples from different models and we can build a predictive distribution from these samples. Below is a plot representing the quality of uncertainty with a model identical to the one we used for VI.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/uncertainty/MCDrop.png&quot; alt=&quot;MCDrop&quot; title=&quot;Simple Transfer&quot; /&gt;
&lt;figcaption&gt;97.06% accuracy with 0.5 $p$ and 1e-5 L2&lt;/figcaption&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The results are slightly better than VI. A possible explanation is the absence of the mean field approximation and we may also be sampling from richer posteriors of weights. But in that case, one can expect much better results. The authors in the MC dropout paper talks about the determinants ofpredictive uncertainty - model structure, model prior and approximating distribution. They identify that the predictive uncertainty depends heavily on the non-linearity and model prior length-scale(equivalent to weight decay). This was proven true when I used ReLu and adjusted the L2 regularisation and dropout probability $p$ and got a somewhat desirable result with 99.3% accuracy(uncertainty plotted below). However that was more ad hocery than I’m comfortable with as techniques to ‘learn’ these hyper parameters should in theory only lead to overfitting and over-confidence(&lt;a href=&quot;https://arxiv.org/pdf/1703.01961&quot;&gt;Louizos and Welling(2017)&lt;/a&gt;). The hyperparameters(from trial and error) for the best result also contradict the fact that 0.5 for $p$ provides the most variance in weights which will translate into greater variance in the predictive distribution (&lt;a href=&quot;https://www.cs.toronto.edu/%7Ehinton/absps/JMLRdropout.pdf&quot;&gt;Srivastava et al.(2014)&lt;/a&gt;). Most of the models in use are ones that are tuned for accuracy and may not perform as well if we ask it for uncertainty estimates. Thus, its another step for the user to search for a model that not only gives good accuracy but also appropriate uncertainty estimates.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/uncertainty/MCDropB.png&quot; alt=&quot;MCDropB&quot; title=&quot;High accuracy(.99) and uncertainty&quot; /&gt;
&lt;figcaption&gt;High accuracy(99.2%)/confidence on MNIST and low confidence on nMNIST&lt;/figcaption&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Note that this is in no way a criticism of MC dropout, it’s simply pointing out gaps in my individual or our collective knowledge. Nevertheless, its also worth pointing out at an actual criticism of the method in a &lt;a href=&quot;http://bayesiandeeplearning.org/2016/papers/BDL_4.pdf&quot;&gt;note&lt;/a&gt; by Ian Osband which raises a confusion between risk and model uncertainty and posits that MC dropout is actually approximating risk which if it is the case, shouldn’t be the uncertainty we should be putting our money on. I feel that this is a valid question and it requires more research hours especially into the theoretical aspects to come to a conclusion.&lt;/p&gt;

&lt;h2&gt;Coming up&lt;/h2&gt;
&lt;p&gt;There are several other exciting methods to uncertainty estimation which includes propositions from the Bayesian paradigm such as Normalizing Flows, Hamiltonian Monte Carlo as well as frequentist methods such as bootstrap sampling, ensembles and more. I’ll update the post irregularly as I get around to exploring the related papers. Given below is a an empirical CDF of the entropy of predictive distributions as done in Louizos and Welling(2017). This should provide a good comparitive picture of the methods implemented so far. Curves that are closer to the bottom right part of the plot are preferable for nMNIST, as it denotes that the probability of observing a high confidence prediction is low and the opposite for MNIST.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/uncertainty/cdf.png&quot; alt=&quot;CDF&quot; title=&quot;Empirical CDF of Entropy&quot; /&gt;
&lt;figcaption&gt;Empirical CDF of entropy in nMNIST(L) and MNIST(R)&lt;/figcaption&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;-&lt;a href=&quot;https://www.tensorflow.org/versions/r1.1/get_started/mnist/pros#train_and_evaluate_the_model&quot;&gt;Tensorflow MNIST tutorial&lt;/a&gt;&lt;br /&gt;
-&lt;a href=&quot;http://www.wcas.northwestern.edu/nescan/knill.pdf&quot;&gt;The Bayesian brain: the role of
uncertainty in neural coding and
computation, Knill and Pouget, 2004&lt;/a&gt;&lt;br /&gt;
-&lt;a href=&quot;https://www.alpha-i.co/blog/MNIST-for-ML-beginners-The-Bayesian-Way.html&quot;&gt;MLP in Edward Tutorial&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Mar 2018 11:41:31 +0530</pubDate>
        <link>http://localhost:4000/uncertainty/bayesian/neural/networks/2018/03/30/uncertainty_dl.html</link>
        <guid isPermaLink="true">http://localhost:4000/uncertainty/bayesian/neural/networks/2018/03/30/uncertainty_dl.html</guid>
        
        
        <category>Uncertainty</category>
        
        <category>Bayesian</category>
        
        <category>Neural</category>
        
        <category>Networks</category>
        
      </item>
    
      <item>
        <title>Gaussian Process with PyMC3</title>
        <description>&lt;p&gt;The goal is to explore &lt;b&gt;Gaussian&lt;/b&gt; process(GP) which are &lt;b&gt;Bayesian&lt;/b&gt; &lt;b&gt;non-parametric&lt;/b&gt; models, in the context of &lt;b&gt;regression&lt;/b&gt; problems.&lt;/p&gt;

&lt;p&gt;Focusing on the key terms, the easiest to tackle is regression which you may already know given that you wanted to know more about Gaussian processes. In the most simplest terms, regression is used to model and predict continuous values. In our adventure with Gaussian process, we will be working on predicting winning times for olympic events(eg. 100m dash) given an year, learning a mapping between the two.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/100mds.png&quot; alt=&quot;OlympicsDataset&quot; title=&quot;Olympics 100m Mens&quot; /&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Please find the &lt;a href=&quot;https://github.com/shehel/Bayesian_analysis/blob/master/GP_regression.ipynb&quot;&gt;iPython notebook&lt;/a&gt; to accompany this tutorial and check out references for some excellent resources. We will be using the PyMC3 python library for probabilistic programming, i.e., implementing GP.&lt;/p&gt;

&lt;h2&gt;Channeling the Bayesian&lt;/h2&gt;
&lt;p&gt;The key idea is that all Bayesian answers to questions are desribed in probabilities. And that means, you are handed a distribution of results representing the uncertainty inherent in the problem. Ofcourse, one is free to choose the ‘best’ from this distribution but lets try to understand why thinking like a Bayesian is so natural and institutionalised .&lt;/p&gt;

&lt;p&gt;Imagine this, you forgot your watch and would like to know the time in hours. The only source of information is your trusty thermometer and your past experience with correlating thermometer readings to time of the day, it should be obvious that there’s no one to one mapping here - there is some uncertainty. It shows a fairly high reading and immediately you narrow down your answer. But notice that you don’t home in on a single value but a range of value weighted by your past experience - you might think its the afternoon, somewhere between 10:00 and 15:00 with a greater weighting near 12:00 because your past experience has shown you that it corresponded to high temperature readings. If the above described your thought process, congratulations! You have proved you are a bayesian and a human aka &lt;u&gt;not&lt;/u&gt; a flawless all knowing super intelligence. Thinking like a Bayesian, an individual, most likely Thomas Bayes but Richard Price and Laplace are also plausible, codified the notion of updating weightage with evidence through&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underbrace{P(A|B)}_{Posterior} = \frac{\underbrace{P(B|A)}_{Likelihood} * \underbrace{P(B)}_{Prior} }{\underbrace{P(A)}_{Evidence/Marginal Likelihood}}&lt;/script&gt;

&lt;p&gt;Our brains handle regression implicitly but computers require an explicit description of this using weight parameters over inputs, which in our case is temperatures. But you already know this, if not, head to the nearest linear regression tutorial. We are now ready to go infinite parameters.&lt;/p&gt;

&lt;h2&gt;Non-parametric == infinite parameters&lt;/h2&gt;
&lt;p&gt;If you tried to understand ‘Non-parametric’ literally, you are not alone and sorry to dissappoint. As cool as such an idea may sound, its also not a thing yet. Bayesian non-parametric methods do not imply that there are no parameters since probability distributions need parameters, but the number of parameters grow with the size of the dataset. Thus, Bayesian non-parametric models are free birds with infinite parameters.&lt;/p&gt;

&lt;p&gt;The parametric approach
has an obvious problem in that we have to decide upon the freedom of the class
of functions considered; if we are using a model based on a certain class of
functions (e.g. linear functions) and the target function is not well modelled by
this class, then the predictions will be poor. We could increase the
flexibility of the class of functions, but this runs into the danger of overfitting,
where we can obtain a good fit to the training data, but perform badly when
making test predictions. The non-parametric approach appears to have a serious problem, in that surely
there are an uncountably infinite set of possible functions, and how are we
Gaussian process going to compute with this set in finite time? Lets find out.&lt;/p&gt;

&lt;h2&gt;Come get your Gaussian&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x \mid \pi, \Sigma) = (2\pi)^{-k/2}|\Sigma|^{-1/2} \exp\left\{ -\frac{1}{2} (x-\mu)^{\prime}\Sigma^{-1}(x-\mu) \right\}&lt;/script&gt;

&lt;p&gt;Gaussians can be applied almost everywhere, not because of Central Limit Theorem and that everything follows a Normal distribution, which is not true since its mostly the case that they follow a distribution close to a gaussian and not exactly a perfect Gaussian. So why bother with a complicated distribution when you can reasonably approximate with a computationally gifted gaussian. Gifted because it has closure under multiplication, linear projection, marginalisation and conditioning.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_1 \in \mathbb{R}^p&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_2 \in \mathbb{R}^q&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p(x_1,x_2) = \mathcal{N}\left(\left[{
\begin{array}{c}
  {\mu_1}  \\
  {\mu_2}  \\
\end{array}
}\right], \left[{
\begin{array}{c}
  {\Sigma_{11}} &amp; {\Sigma_{12}}  \\
  {\Sigma_{12}^T} &amp; {\Sigma_{2}}  \\
\end{array}
}\right]\right) %]]&gt;&lt;/script&gt;

&lt;p&gt;Assume we have a joint distribution as defined above. Note that we are using block matrices where $\Sigma$ is a $n*n$ matrix where $n=p+q$. The two key properties to remember are Gaussian under conditioning&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_1 \mid x_2) = \mathcal{N}(\mu_1 + \Sigma_{12}\Sigma_2^{-1}(x_2-\mu_2), 
\Sigma_1-\Sigma_{12}\Sigma_2^{-1}\Sigma_{12}^T) \label{conditioning} \tag{1}&lt;/script&gt;

&lt;p&gt;This can be derived by rewriting the joint as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_1, x_2) = p(x_1\mid x_2)p(x_2)&lt;/script&gt;

&lt;p&gt;and simply dropping the second term. An obstacle is the inversion of $\Sigma$. We can make use of the &lt;a href=&quot;http://www.cis.upenn.edu/~jean/schur-comp.pdf&quot;&gt;Schur compliment&lt;/a&gt; to obtain the following factorisation for $\Sigma^{-1}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
I &amp; O \\
-\Sigma_{2}^{-1}\Sigma_{12}^T &amp; I
\end{bmatrix}
\begin{bmatrix}
(\Sigma_{11}-\Sigma_{12}\Sigma_{2}^{-1}\Sigma_{21}^T)^{-1} &amp; O \\
O &amp; \Sigma_{2}^{-1}
\end{bmatrix}
\begin{bmatrix}
I &amp; -\Sigma_{12}\Sigma_{2}^{-1} \\
O &amp; I
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The covariance of the new conditional Gaussian is the Schur compliment $(\Sigma_{11}-\Sigma_{12}\Sigma_{2}^{-1}\Sigma_{12}^T)^{-1}$. The mean can be obtained by matrix multiplication of the outer terms and splitting into two Gaussians, one for $p(x_1\mid x_2)$ and the other for $p(x_2)$.&lt;/p&gt;

&lt;p&gt;And Gaussian under marginalisation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_1) = \int p(x_1,x_2) dx_2 = \mathcal{N}(\mu_1, \Sigma_1) \label{marginal} \tag{2}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_2) = \int p(x_1,x_2) dx_1 = \mathcal{N}(\mu_2, \Sigma_2)&lt;/script&gt;

&lt;p&gt;which can be derived by marginalizing over $x_1$, we can pull the second exponential term $p(x_2)$ outside the integral, and the first term is just the density of a Gaussian distribution, so it integrates to 1 and we are left with a trivial marginal.&lt;/p&gt;

&lt;h2&gt;Gaussian Process&lt;/h2&gt;
&lt;p&gt;Think of Gaussian process as a method that allows the data to speak for itself rather than external forces controlling the hyperparameters(complexity), which essentially allows the model to learn complex functions as more data becomes available. A Gaussian process generalizes the multivariate normal to infinite dimension. It is defined as an infinite collection of random variables, any finite subset of which have a Gaussian distribution. While a probability distribution describes random vectors, a process describes functions. Marginalisation property \ref{marginal} of Gaussians is the reason why we can go infinite. Thus, each of our datapoints are dimensions of a multivariate Gaussian. Following the logic, we can marginalize over the infinitely-many variables we are not interested in to find the value of the variable we actually need.&lt;/p&gt;

&lt;p&gt;We can describe a Gaussian process as a disribution over functions. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP is fully specified by a mean function and a covariance function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) \sim \mathcal{GP}(m(x), K(x,x^{\prime}))&lt;/script&gt;

&lt;p&gt;A widely used GP takes 0 for mean and uses SE kernel as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
m(x) &amp;=0 \\
K(x,x^{\prime}) &amp;= \theta exp( \frac{-(x_i-x_j)^2}{2l^2})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The covariance function(kernel) is a RBF(Radial Basis Function) or SE(Squared Exponential), for which values of $x$ and $x^{\prime}$ that are close together result in values of $K$ closer to 1 and those that are far apart return values closer to zero.&lt;/p&gt;

&lt;p&gt;To clarify things a bit, it is common but by no means necessary to consider GPs with a zero mean function. Most of the learning occurs in the covariance function and specifying a mean which is hard to specify is not necessary.&lt;/p&gt;

&lt;p&gt;RBF is very thoroughly studied concept and $ls$ and $\theta$ are its hyperparameter. The SE(squared exponential) kernel as it is known in the GP community can be thought of as measuring the similarity between $x$ and $x^{\prime}$. The covariance function is calculated among all combination of $x_n$ and $x^{\prime}_n$ and we get a $n*n$ matrix. $ls$ is an interesting variable as it defines the shape of the gaussian shaped SE. A low gamma corresponds to very low tolerance in that an $x^{\prime}_n$ needs to be very close to be considered ‘similar’. $\theta$ is the maximum allowable covariance, kind of acting like a weight on the values.&lt;/p&gt;

&lt;p&gt;Role of $ls$ is illustrated with the plots below. I have chosen $x_n$ to be constant at 0 and $x^{\prime}_n$ range from -5 to 5. From left to right, the plots have $ls$ 10, 0 and 0.1.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/gammas.png&quot; alt=&quot;gammas&quot; title=&quot;Varying Gammas&quot; /&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean functon and covariance function evaluated at those points.&lt;/p&gt;

&lt;p&gt;The behavior of individual realizations from the GP is governed by the covariance function. This function controls both the degree of shrinkage to the mean function and the smoothness of functions sampled from the GP.&lt;/p&gt;

&lt;h2&gt;Distribution over Functions&lt;/h2&gt;
&lt;p&gt;Below is a plot containing 20 &lt;u&gt;functions&lt;/u&gt; sampled from a GP. When we write a function that takes continuous values as inputs, we are essentially specifying an infinte vector that only returns values (indexed by the inputs) when the function is called upon to do so. GPs are non-parametric due to this behaviour and the nice properties of a Gaussian makes this possible. This idea may seem a bit alien and I assure you that the pieces will fall in place when we apply it to a regression problem.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/gps20.png&quot; alt=&quot;sampled&quot; title=&quot;Sampling from a GP&quot; /&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;h2&gt;Noiseless Regression&lt;/h2&gt;
&lt;p&gt;Here, we need the other important ingredient for GPs, closure under conditioning of Gaussians. The conditioning formula \ref{conditioning} provides an analytical solution to finding the posterior that depends on inverting a single matrix. In the process of going from our prior to the posterior, we are revising our uncertainties based on where data is present. Applying the equation for data points sampled from a cos function and sampling functions from the posterior GP, we get a plot like below. It shows 500 functions sampled from a GP all of which are reasonable explanations derived from our dataset.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/posteriorGP.png&quot; alt=&quot;sampledPosterior&quot; title=&quot;Sampling from a Posterior GP&quot; /&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The points were different functions see to converge are the observed data, uncertainty happens to be low here. What about the case when there is noise in the data, i.e., in the case when&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = f(x) + N(0,\sigma^2)&lt;/script&gt;

&lt;p&gt;The equations answers itself because remember, $f(x)$ is a gaussian and adding noise which is also a Gaussian is a trivial with normally distributed independent Gaussian variables, it is simply $\sigma^2$ added to $K$. A more nagging question is about the hyperparameters (length scale, $\sigma^2$ etc) we’ve taken for granted, how on Earth do we find them? An analytic solution is  rare because in most cases, a data generating function can’t be modelled neatly with Gaussians everywhere in a heirarchical specification of models. That brings us to the next section on inference and probabilistic programming for estimating hyperparameters.&lt;/p&gt;

&lt;h2&gt;Inference&lt;/h2&gt;
&lt;p&gt;We have 3 methods for finding our hyperparameters at this juncture, the more frequentist Empirical Bayes and Cross Validation or full Bayesian inference of hyperparameters with approximating techniques using PyMC3. You already know where we are headed and I do not pursue the other option here as it is kind of obsolete(imho) now that we have efficient probabilistic techniques. An added benefit is that we can go over the wonderful Occam’s Razor property of Bayesian Models.&lt;/p&gt;

&lt;p&gt;We have a dataset of Olympics mens 100m winning times for the past century and would like to infer a patters through fitting a GP. Given below is the result.&lt;/p&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/100mgp.png&quot; alt=&quot;OlympicsDatasetGP&quot; title=&quot;Olympics 100m Mens Posterior&quot; /&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Occam’s Razor also known  as the “law of parsimony” is the problem solving principle that, when presented with competing hypothetical answers to a problem, one should select the one that makes the fewest assumptions. Imagine this, with our infinitely complex GP, it’s not hard to imagine models that totally overfit with very complicated functions. As seen in the figure above, this doesn’t happen because Occam’s Razor is baked into the Bayes formula, namely the  &lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall09/cos597A/papers/MacKay2003-Ch28.pdf&quot;&gt;marginal likelihood term&lt;/a&gt;. This is important for us because inferring hyperparameters essentially boils down to comparing models with different parameters and choosing the ones that explains the data the best. Let’s see the information we have and conclusions we can reach in the bayesian setting -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Likelihood term which is the probability of the data given our function $f$ and model description $\mathcal{M}$ which includes the hyperparameters we seek $p(y|f, \mathcal{M})$.&lt;/li&gt;
&lt;li&gt;A prior $p(f|\mathcal{M})$ over $f$.&lt;/li&gt;
&lt;li&gt;We now require the posterior over our hyperparameters. This is an example of Hierarchical Bayes and we could go on and on with priors over priors and its neat that we can find posteriors over all of them but that's all encapsulated here in $\mathcal{M}$. Recollect the &lt;b&gt;Marginal Likelihood&lt;/b&gt; which here allows us to get rid(marginalise) of the seemingly infinite $f$.

$$p(y|\mathcal{M}) = \int p(y|f, \mathcal{M})p(f| \mathcal{M})df$$

Both the terms being Gaussian, this is an easy integral.&lt;/li&gt;

&lt;li&gt;We can simply apply the Bayes Theorem to get back our posterior on hyperparameters.

$$p(\mathcal{M}|y) = \frac{p(y|\mathcal{M}p(\mathcal{M}))}{p(y)}$$
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This can all be done with PyMCs &lt;code class=&quot;highlighter-rouge&quot;&gt;gp.marginal_likelihood&lt;/code&gt; method. We can get the predictive posterior by conditioning the new dataset on observed data by using &lt;code class=&quot;highlighter-rouge&quot;&gt;gp.conditional&lt;/code&gt; which we have covered already. There is also a &lt;code class=&quot;highlighter-rouge&quot;&gt;gp.predict&lt;/code&gt; method that takes in data and computes the $y$s given a point.&lt;/p&gt;

&lt;h2&gt;Model Design&lt;/h2&gt;
&lt;p&gt;The effectiveness of a GP and similarly, the appropriateness of the functions it realizes completely lies with the choice of the covariance kernel. Fortunately, PyMC3 includes a large library of covariance functions to choose from. Furthermore, you can also combine covariance functions to model heterogenous data or even come up with a new shiny kernel of your own.&lt;/p&gt;

&lt;p&gt;We also have a major constraint with GPs in that it requires the inversion of $K$ when calculating the posterior. This quickly becomes unscaleable when fitting large datasets. A reasonable workaround is building a sparse approximation of the covariance matrix by not creating a full covariance matrix over all $n$ training inputs. This method brings down the complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(nm^2)$ where $m&amp;lt;n$. Although sparse approximation may reduce the expressiveness of the GP, clever choice of $m$ can potentially rectify this. PyMC has &lt;code class=&quot;highlighter-rouge&quot;&gt;gp.MarginalSparse&lt;/code&gt; that implements sparse approximation for which there’s an example in the accompanying notebook. However, this is an interesting field to follow up on.&lt;/p&gt;

&lt;p&gt;PyMC also provides a &lt;code class=&quot;highlighter-rouge&quot;&gt;gp.Latent&lt;/code&gt; method that has a more general implementation of a GP. You can relax the assumption that the data is normally distributed and work with other distributions. But I couldn’t think of a problem where this would work better than regular GPs.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;The goal of this post was to introduce a very powerful non-parametric technique to reason about data. We went over the key ingredient of GPs, closure properties of Gaussians and methods of deriving it, finally using the knowledge to apply it to a regression problem using PyMC python library. We also went over inferring hyperparameters of a GP using the marginal likelihood method. Thus, we were able to fit a curve/line to a dataset without overfitting.&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;GP is a very mature field of research with a wide variety of practical applications from geostatistics to finding the hyperparameters of neural networks. Thus, we have only seen the tip of the iceberg and I encourage exploring the topic.&lt;/p&gt;

&lt;p&gt;For an immediate follow up task, I would highly recommend finding a dataset and do regression on it using GPs. The more complex the dataset, the better such as large datasets, multidimensional datasets etc. There are several Kernel functions to try out aswell. Another interesting usage is balancing the exploitation-exploration tradeoff in Bayesian Optimisation whereby the model learns optimal choices given mean and uncertainty. Although I made no mention, GPs also found success(to an extent) in classification tasks. If you don’t feel too Bayesian, try empirical bayes and/or cross validation for which &lt;a href=&quot;http://scikit-learn.org/stable/modules/gaussian_process.html&quot;&gt;scikit-learn&lt;/a&gt; might be useful.&lt;/p&gt;

&lt;p&gt;Please check the references for pointers.&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/fonnesbeck/gp_tutorial_pydata&quot;&gt;A great practical tutorial series on GP using PYMC&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://www.gaussianprocess.org/gpml/&quot;&gt;The Bible of GP&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.cs.ubc.ca/~murphyk/MLbook/&quot;&gt;An all round great book for probabilistic ML by Kevin Murphy&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;www.inference.org.uk/itprnn/book.pdf &quot;&gt;Another great book on Bayesian ML by MacKay&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://www.cis.upenn.edu/~jean/schur-comp.pdf&quot;&gt;Proof of Schur Compliments&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 10 Dec 2017 10:44:31 +0530</pubDate>
        <link>http://localhost:4000/gaussian/processes/bayesian/non-parametric/pymc3/2017/12/10/gp_pymc3.html</link>
        <guid isPermaLink="true">http://localhost:4000/gaussian/processes/bayesian/non-parametric/pymc3/2017/12/10/gp_pymc3.html</guid>
        
        
        <category>Gaussian</category>
        
        <category>Processes</category>
        
        <category>Bayesian</category>
        
        <category>Non-parametric</category>
        
        <category>PyMC3</category>
        
      </item>
    
      <item>
        <title>Sum Product Algorithm and Graph Factorization</title>
        <description>&lt;p&gt;This post describes the Sum Product algorithm which is a neat idea that leverages a handful of important theories from across the field and extends towards more complex models. I found it to be a very nice method for accessing more complex models and it is infact a generalization of  Markov Chains, Kalman Filter, Fast Fourier Transform, Forward-Backward algorithm and more. I’m aware that the approach I use may not satisfy everyone and in that case, take a look at the reference section for some excellent resources.&lt;/p&gt;

&lt;h2&gt;Minimum Minimorum&lt;/h2&gt;
&lt;p&gt;I assume familiarity with the 2 fundamental rules of probability, the product rule and the sum rule given below and the associated notions of joint distributions and marginals. We will be making extensive use of these.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(X,Y)=p(Y|X)p(X) \label{product} \tag{1}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(X) = \sum_y p(X,Y) \label{sum} \tag{2}&lt;/script&gt;

&lt;h2&gt;What and Why - Sum Product Algorithm?&lt;/h2&gt;
&lt;p&gt;It is used for Inference, which is a frequently used word in statistics to mean marginalizing a joint distribution so we can be informed of something that was unknown given the other known variables. 
An issue with marginalizing a joint is that it quickly becomes intractable, i.e., computationally impossible due to the size of the numbers involved. For instance, say you have a model with 100 binary variables (each variable is 0 or 1) and you are now faced with marginalizing a joint distribution with $2^{100}$ terms. Here we can make a simplifying assumption that our joint distribution is not exactly a general joint but a factorized distribution where each of the factors just depend on a few local variables that are ‘close’ to it. This is the underlying assumption that makes fast inference through Sum Product possible.&lt;/p&gt;

&lt;h2&gt;Factorization&lt;/h2&gt;
&lt;p&gt;I have been mentioning factors and all it means here is that there’s a different way of specifying joint distribution wherein the function (joint distribution) $p(x_1,…,x_n)$ factors into a product of several local functions each of which only contain a subset of the variables.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_1,...,x_n) = \prod_s f_s(X_s)&lt;/script&gt;

&lt;p&gt;where 
  $X_s$ is a subset of the variables.&lt;/p&gt;

&lt;h2&gt;Factor Graphs&lt;/h2&gt;
&lt;p&gt;Factorization can be verbosely represented through factor graphs because it makes it explicit by adding additional nodes for factors.&lt;/p&gt;
&lt;div style=&quot;example&quot;&gt;
	&lt;pre&gt;
A factor graph is a bipartite graph that expresses the structure&lt;br /&gt;of the factorization.A factor graph has a variable node for each variable $x_i$,&lt;br /&gt;a factor node for each local function $f_s$, and an edge-connecting variable node&lt;br /&gt;$x_i$ to factor node $f_s$ if and only if $x_i$ is an argument of $f_s$.
	&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;An example factorization and its corresponding graph is given below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(a, b, c, d, e) = f_1(a, b) f_2(b,c,d) f_3(d, e)&lt;/script&gt;

&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/factorised.png&quot; alt=&quot;factorgraph&quot; title=&quot;
Factor Graph&quot; /&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;h2&gt;Factor graphs as Expression Trees&lt;/h2&gt;

&lt;p&gt;If the factor graph doesn’t contain cycles, then it can be represented as a tree and computation can be simplified using the distributive law of multiplication - \ref{distributive}. We can view this with a ‘message-passing’ analogy whereby the marginal variable is the ‘product’ of ‘messages’. This idea is made clear in the next section. To convert a function representing $p(x_1,…,x_n)$ to the corresponding expression tree for $p(x_i)$, rearrange the factor graph as a rooted tree with $x_i$ as root.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{split}
\sum_x \sum_y xy = x_1y_1+x_2y_1+x_1y_2+x_2y_2 \\
= (x_1+x_2)(y_1+y_2) 
= \sum_x x \sum_y y 
\end{split} \label{distributive} \tag{3}&lt;/script&gt;

&lt;h2&gt;Algorithm in action&lt;/h2&gt;
&lt;p&gt;I found that the best way to learn the algorithm was to see it in execution given the basic ingredients we have gathered so far. There are missing pieces which will be explained as it appears.&lt;/p&gt;

&lt;h3&gt;Problem - Find the marginal&lt;/h3&gt;
&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/workfac.png&quot; alt=&quot;factorgraph2&quot; title=&quot;
Factor Graph&quot; /&gt;&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;The factor graph describes the factorization given&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(a,b,c,d,e) = f_1(a)f_2(b)f_3(a,b,c)f_4(c,d)f_5(c,e) \label{cc} \tag{4}&lt;/script&gt;

&lt;p&gt;And we want to find the marginal $p(c)$. Using \ref{sum}&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(c) = \sum_a \sum_b \sum_d \sum_e p(a, b, c, d, e) \label{dd} \tag{5}&lt;/script&gt;

&lt;p&gt;We can represent the factor graph as a tree and this will give us the ground to build an intuition of message passing. Notice that the tree is simply rearranged to reflect our problem of finding $p(c)$.&lt;/p&gt;
&lt;figure&gt;&lt;center&gt;
&lt;img src=&quot;/assets/worktree.png&quot; alt=&quot;factorgraphtree&quot; title=&quot;Factor Graph as a Tree&quot; /&gt;&lt;/center&gt;
&lt;/figure&gt;

&lt;h3&gt;Message Passing&lt;/h3&gt;
&lt;p&gt;Using the message passing analogy, picture the marginal as a message comprised of several messages that were gathered along the branches of the tree. Substituting \ref{cc} in \ref{dd}, we get a form that we can start to work on&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(c) = \sum_a \sum_b \sum_d \sum_e f_1(a)f_2(b)f_3(a,b,c)f_4(c,d)f_5(c,e)&lt;/script&gt;

&lt;ol&gt;&lt;li&gt;
Variable $c$ is composed of 3 messages that it received from each of its neighboring factors. To get the message of a variable node, simply multiply all the incoming messages from neighboring factor nodes.

$$p(c) = m_{f3 \rightarrow c}(c) m_{f4 \rightarrow c}(c) m_{f5 \rightarrow c}(c)$$

where $m_{x \rightarrow y}(z)$ represents a message sent from node $x$ to node $y$ which is a function of variable $z$ because the other variables have been summed out. &lt;/li&gt;
&lt;li&gt;A natural question now is what the message in the factor nodes are. Let's go through the first factor. In case you are left wondering how something is the way it is, remember that everything is a combination of \ref{distributive} and \ref{sum}. To initiate messages at leaf nodes, the following rules are used depending on if its a leaf or factor node:

&lt;ul&gt;
	&lt;li&gt;$m_{x \rightarrow f}(x)=1$ a leaf variable node sends an identity function.
	&lt;/li&gt;
	&lt;li&gt;$m_{f \rightarrow x}(x)=f(x)$ a leaf factor node sends a description of the function to its parent
	&lt;/li&gt;
&lt;/ul&gt;

The 'procedure' to evaluate a message send by a factor node:
&lt;ol&gt;
	&lt;li&gt;Take product of incoming messages into factor node. 
	&lt;/li&gt;
	&lt;li&gt;Multiply by factor associated with the node&lt;/li&gt;
	&lt;li&gt;Marginalize over all variables associated with incoming messages by pulling out the summations. 
	&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By recursively applying the two rules we have seen, the two incoming messages for $f_3$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_{a\rightarrow f3}(b) = f_1(a)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_{b \rightarrow f3}(a) = f_2(b)&lt;/script&gt;

&lt;p&gt;Note that the right hand side of both the above equations can be seen as the message from the factor node since variable node simply multiply the messages of factor nodes.And what we are left with is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_{f3 \rightarrow c}(c) = \sum_a f_1(a) \sum_b f_2(b) \Big[ f3(a,b,c) \Big] \label{3-c} \tag{6}&lt;/script&gt;

&lt;p&gt;In the original paper, the authors propose a different notation for equations like above called ‘not-sum’ or summary notation which gives \ref{3-c} the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_{f3 \rightarrow c}(c) = \sum_{\sim c} f_1(a) f_2(b) f3(a,b,c)&lt;/script&gt;

&lt;div class=&quot;example&quot;&gt;
&lt;pre&gt;
Instead of indicating the variables being summed over, we indicate&lt;br /&gt;those variables not being summed over. 
&lt;/pre&gt;
&lt;/div&gt;

&lt;div style=&quot;example&quot;&gt;
The Sum-Product Update Rule:
&lt;pre&gt;
The message sent from a node $v$ on an edge $c$ is the product of the&lt;br /&gt;local function at $v$(or the unit function if $v$ is a variable node) with&lt;br /&gt;all messages received at $v$ on edges other than $e$, summarized for&lt;br /&gt;the variable associated with $e$.

Variable to local function

$$m_{x \to f} (x) = \prod_{h \in n(x) \backslash \{f\}} m_{h \to x} (x)$$

Local function to variable

$$m_{f \to x} (x) = \sum_{\sim \{x\}} \Big(
f(X) \prod_{y \in n(f) \backslash \{x\}} m_{y \to f} (y)
\Big)$$

&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We now know everything required to complete the marginal. Applying the equations above, the final form is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(c) = \sum_{\sim c} \Big(f_1(a) f_2(b) f3(a,b,c) \Big) \sum_{\sim c}\Big(f_4(d)\Big) \sum_{\sim c}\Big(f_5(e)\Big)&lt;/script&gt;

&lt;h2&gt;Marginal for every node&lt;/h2&gt;
&lt;p&gt;Having done the work of finding a marginal for $p(x)$, and going on to think of calculating the marginal for $p(y)$ both variables in the same factor graph, one might notice redundancies because the sub computations for evaluating messages are the same. We can take advantage of this by picking any node and propagating messages from leaf to the root as shown above and from the root back to the leaf so that every node has seen two messages, caching the evaluations all along. A slight increase in computation but now we have all the marginals.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully this post introduced Factor Graphs and Sum Product algorithm and provided an intuition of the ideas. It is essentially a method for exact inference of the marginal given a factorized distribution in an acyclical graph.&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;I strongly recommend reading the &lt;a href=&quot;http://www.psi.toronto.edu/pubs/2001/frey2001factor.pdf&quot;&gt;original paper&lt;/a&gt; as this post introduced less than half of the paper. It’s very accessible with familiar examples and contains a lot more information including applications.&lt;/p&gt;

&lt;p&gt;I also avoided talking about cyclic graphs which is where most of the interesting problems lie and the paper discusses interesting ways of working around this with algorithms like Junction Tree and Loopy Belief but you already know everything to understand them.&lt;/p&gt;

&lt;p&gt;Another avenue for thought is working with continuous variables, i.e., when the messages are intractable. Interesting techniques like Monte Carlo and Variational inference are used in such cases which are whole books worth of content on its own.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ilyakava/sumproduct&quot;&gt;Here&lt;/a&gt; is a nice python implementation of the algorithm for the code savvy learners.&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://www.psi.toronto.edu/pubs/2001/frey2001factor.pdf&quot;&gt;Factor Graphs and the Sum-Product Algorithm&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=c0AWH5UFyOk&amp;amp;t=3516s&quot;&gt;Christopher Bishop’s presentation video&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/ilyakava/sumproduct&quot;&gt;Ilya’s Sum Product Python implementation&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://lipn.univ-paris13.fr/~dovgal/about.html&quot;&gt;Sergey Dovgal’s post&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 30 Nov 2017 09:45:31 +0530</pubDate>
        <link>http://localhost:4000/graphical/modeling/2017/11/30/prod_sum_algo.html</link>
        <guid isPermaLink="true">http://localhost:4000/graphical/modeling/2017/11/30/prod_sum_algo.html</guid>
        
        
        <category>Graphical</category>
        
        <category>Modeling</category>
        
      </item>
    
      <item>
        <title>MathJax with Jekyll</title>
        <description>&lt;p&gt;I use &lt;a href=&quot;https://github.com/hemangsk/Gravity&quot;&gt; this &lt;/a&gt;simple jekyll theme by hemangsk. Amongst several methods floating in the internet, here is what worked for me with kramdown to get latex running with jekyll pages.&lt;/p&gt;

&lt;p&gt;Add to &lt;code class=&quot;highlighter-rouge&quot;&gt;_layouts/post.html&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Add to &lt;code class=&quot;highlighter-rouge&quot;&gt;_includes/head.html&lt;/code&gt; between the &lt;code class=&quot;highlighter-rouge&quot;&gt;head&lt;/code&gt; tags&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text/x-mathjax-config&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;MathJax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Hub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;tex2jax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;skipTags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'script'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'noscript'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'style'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'textarea'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;inlineMath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;skipTags&lt;/code&gt; specify blocks where latex will be disabled. Code between &lt;code class=&quot;highlighter-rouge&quot;&gt;$$&lt;/code&gt; will be inline.&lt;/p&gt;
</description>
        <pubDate>Thu, 13 Jul 2017 01:12:31 +0530</pubDate>
        <link>http://localhost:4000/mathjax/with/jekyll/2017/07/13/mathjaxjekyll.html</link>
        <guid isPermaLink="true">http://localhost:4000/mathjax/with/jekyll/2017/07/13/mathjaxjekyll.html</guid>
        
        
        <category>MathJax</category>
        
        <category>with</category>
        
        <category>jekyll</category>
        
      </item>
    
  </channel>
</rss>
